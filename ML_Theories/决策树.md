# 决策树模型
分类决策树模型是表示基于特征对实例进行分类的树形结构。
决策树可以转换为一个if-then规则的集合，也可以看做是定义在特征空间划分上的类的条件概率分布。其中，每一个叶节点就表示一个类。
# 决策树学习
决策树学习的目的是构建一个与训练数据拟合很好，并且复杂度小的决策树。
由于从可能的决策树中直接选择最优决策树是NP完全问题，现实中常采用启发式方法学习次优的决策树。什么是启发式方法？？？
# 决策树算法
决策树算法由三部分构成：
- 特征选择
- 树的生成
- 树的剪枝

常用的算法由ID3, C4.5，CART

## 特征选择
特征选择部分中往往只留下对训练数据有足够分类能力的特征。
常用的特征选择的准则有：
### 信息增益(ID3)
其表示的是样本集合D对特征A的信息增益, 其被用于ID3算法中。

![](./img/信息增益.png)

其中，H(D)是数据集D的熵，H(Di)是数据集Di的熵，H(D|A)是数据集D对特征A的条件熵。Di是D中特征A取第i个值的样本子集，Ck是D中属于第K类的样本子集。n是特征A取值的个数，K是类的个数。
### 信息增益比(C4.5)
其表示的是样本集合D对特征A的信息增益比，其被用于C4.5算法中。

![](./img/信息增益比.png)

其中，g(D,A)是信息增益，H(D)是数据集D的熵。
### 基尼指数(CART-classification and regression Trees)
#### 样本集合D的基尼指数

![](./img/样本集合基尼指数.png)

#### 特征A条件下集合D的基尼指数

![](./img/特征基尼指数.png)

## 决策树生成
通常选用信息增益最大、信息增益比最大或基尼指数最小作为特征选择的准则。
决策树的生成往往通过计算信息增益或者是其他指标，从根结点开始，递归地产生决策树。这也就相当于用信息增益或者其他准则不断地选取局部最优的特征，或将训练集分割为能够基本正确分类的子集。
## 决策树的剪枝
由于生成的决策树存在过拟合问题，需要对它进行剪枝，以简化学到的决策树。
决策树的剪枝，往往从已生成的树上减掉一些叶节点或叶节点以上的子树，并将其父结点或者根节点作为新的叶结点，从而简化生成的决策树。
