# 信息熵
1. 一条信息的信息量与它的不确定性有着直接的关系；
2. 信息量的比特数和所有可能情况的对数函log有关

## 信息熵

![](./img/信息熵.png)

# 条件熵
已知X的随机分布P(X), 那么定义在Y下的条件熵为![](./img/条件熵.png)

我们还可以证明： ![](./img/多元条件熵.png)

# 相对熵(交叉熵)
相对熵(Kullback-Leibler Divergence): ![](./img/相对熵.png)
## 三条结论
1. 对于两个完全相同的函数，他们的相对熵为0；
2. 相对熵越大，两个函数的差异越大；
3. 对于概率分布或者概率密度函数，如果取值均大于零，相对熵可以度量两个随机分布的差异性。
